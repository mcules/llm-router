services:
  llama:
    image: ghcr.io/ggerganov/llama.cpp:full
    command: >
      /app/llama-server
      --host 0.0.0.0
      --port 8081
      --router
      --models-dir /models
      --no-webui
    volumes:
      - /models:/models:ro
    networks:
      - llmnode

  node-agent:
    build:
      context: .
      dockerfile: docker/Dockerfile.node-agent
    environment:
      - NODE_ID=node-1
      - SERVER_GRPC_ADDR=router-server:9090
      - LLAMA_BASE_URL=http://llama:8081
      - HOST_MEMINFO_PATH=/host/proc/meminfo
      - HEARTBEAT_SECONDS=1
      - POLL_MODELS_SECONDS=5
      - POLL_SLOTS_SECONDS=1
    volumes:
      - /proc:/host/proc:ro
      - /sys/fs/cgroup:/host/sys/fs/cgroup:ro
    depends_on:
      - llama
    networks:
      - llmnode
      - llmnet

networks:
  llmnode:
    driver: bridge
  llmnet:
    external: false
